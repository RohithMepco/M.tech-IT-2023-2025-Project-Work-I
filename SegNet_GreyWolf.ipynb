{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNxARObrl7Nu",
        "outputId": "ed7e6d2d-9a09-4f72-fdf5-4c637ad2430f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive not mounted, so nothing to flush and unmount.\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n",
        "!rm -rf /content/drive/*\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBtFNPK7mGsO",
        "outputId": "9ec6bf81-a0d4-466d-bab1-9a903b2ec2ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1800 images from /content/drive/MyDrive/Kaggle_Filtered/benign_filtered\n",
            "Loaded 1497 images from /content/drive/MyDrive/Kaggle_Filtered/malignant_filtered\n",
            "Total images loaded: 3297\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Function to load all images from a folder\n",
        "def load_images_from_folder(folder, img_size=(128, 128)):\n",
        "    images = []\n",
        "    for filename in os.listdir(folder):\n",
        "        img_path = os.path.join(folder, filename)\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is not None:\n",
        "            img = cv2.resize(img, img_size)\n",
        "            images.append(img)\n",
        "    print(f\"Loaded {len(images)} images from {folder}\")\n",
        "    return np.array(images)\n",
        "\n",
        "# Paths to the benign and malignant directories\n",
        "benign_dir=\"/content/drive/MyDrive/Kaggle_Filtered/benign_filtered\"\n",
        "malignant_dir=\"/content/drive/MyDrive/Kaggle_Filtered/malignant_filtered\"\n",
        "\n",
        "benign_images = load_images_from_folder(benign_dir)\n",
        "malignant_images = load_images_from_folder(malignant_dir)\n",
        "\n",
        "# Combine the images\n",
        "images = np.concatenate((benign_images, malignant_images), axis=0)\n",
        "\n",
        "# Normalize images (0-1)\n",
        "images = images.astype(np.float32) / 255.0  # Ensure correct dtype for normalization\n",
        "\n",
        "# Shuffle images\n",
        "np.random.shuffle(images)\n",
        "\n",
        "print(f\"Total images loaded: {images.shape[0]}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "almLGC9inf2c"
      },
      "outputs": [],
      "source": [
        "benign_images = benign_images.astype(np.float32) / 255.0\n",
        "malignant_images = malignant_images.astype(np.float32) / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXUVkTkWmX-T",
        "outputId": "f6294245-e69f-4311-c947-1c571ab1bdea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training images shape: (2637, 128, 128, 3), Training masks shape: (2637, 128, 128, 1)\n",
            "Validation images shape: (660, 128, 128, 3), Validation masks shape: (660, 128, 128, 1)\n"
          ]
        }
      ],
      "source": [
        "# Function to generate distinct synthetic masks for benign and malignant images\n",
        "def generate_synthetic_masks(benign_images, malignant_images, threshold=128):\n",
        "    masks = []\n",
        "\n",
        "    # Generate benign masks (diffused or low-intensity masks)\n",
        "    for image in benign_images:\n",
        "        image_uint8 = (image * 255).astype(np.uint8)\n",
        "        gray_image = cv2.cvtColor(image_uint8, cv2.COLOR_BGR2GRAY)\n",
        "        _, mask = cv2.threshold(gray_image, threshold - 30, 255, cv2.THRESH_BINARY)  # Lower threshold for benign\n",
        "        mask = cv2.GaussianBlur(mask, (15, 15), 0)  # Diffuse edges for benign images\n",
        "        mask = cv2.resize(mask, (128, 128))\n",
        "        masks.append(mask / 255.0)\n",
        "\n",
        "    # Generate malignant masks (higher intensity, more focused regions)\n",
        "    for image in malignant_images:\n",
        "        image_uint8 = (image * 255).astype(np.uint8)\n",
        "        gray_image = cv2.cvtColor(image_uint8, cv2.COLOR_BGR2GRAY)\n",
        "        _, mask = cv2.threshold(gray_image, threshold + 30, 255, cv2.THRESH_BINARY)  # Higher threshold for malignant\n",
        "        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((5, 5), np.uint8))  # Enhance edges for malignant\n",
        "        mask = cv2.resize(mask, (128, 128))\n",
        "        masks.append(mask / 255.0)\n",
        "\n",
        "    return np.array(masks).reshape(-1, 128, 128, 1)\n",
        "\n",
        "# Generate synthetic masks for the benign and malignant images\n",
        "lesion_masks = generate_synthetic_masks(benign_images, malignant_images)\n",
        "\n",
        "# Combine benign and malignant images for final dataset\n",
        "images = np.concatenate((benign_images, malignant_images), axis=0)\n",
        "\n",
        "# Shuffle images and masks together\n",
        "shuffled_indices = np.random.permutation(len(images))\n",
        "images = images[shuffled_indices]\n",
        "lesion_masks = lesion_masks[shuffled_indices]\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(images, lesion_masks, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print shapes of training and validation datasets\n",
        "print(f\"Training images shape: {X_train.shape}, Training masks shape: {y_train.shape}\")\n",
        "print(f\"Validation images shape: {X_val.shape}, Validation masks shape: {y_val.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EAYIzZGDmiSP"
      },
      "outputs": [],
      "source": [
        "# 3. Define the updated SegNet model\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "def segnet(input_shape=(128, 128, 3)):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Encoder\n",
        "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
        "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
        "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
        "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
        "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
        "    p3 = layers.MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "    c4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(p3)\n",
        "    c4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c4)\n",
        "    p4 = layers.MaxPooling2D((2, 2))(c4)\n",
        "\n",
        "    # Bottleneck\n",
        "    c5 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(p4)\n",
        "\n",
        "    # Decoder\n",
        "    u1 = layers.UpSampling2D((2, 2))(c5)\n",
        "    u1 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(u1)\n",
        "    u1 = layers.Concatenate()([u1, c4])\n",
        "\n",
        "    u2 = layers.UpSampling2D((2, 2))(u1)\n",
        "    u2 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(u2)\n",
        "    u2 = layers.Concatenate()([u2, c3])\n",
        "\n",
        "    u3 = layers.UpSampling2D((2, 2))(u2)\n",
        "    u3 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u3)\n",
        "    u3 = layers.Concatenate()([u3, c2])\n",
        "\n",
        "    u4 = layers.UpSampling2D((2, 2))(u3)\n",
        "    u4 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u4)\n",
        "    u4 = layers.Concatenate()([u4, c1])\n",
        "\n",
        "    outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(u4)\n",
        "\n",
        "    model = models.Model(inputs=[inputs], outputs=[outputs])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "teLVouCbnJ4U"
      },
      "outputs": [],
      "source": [
        "# 4. Define Grey Wolf Optimizer\n",
        "class GreyWolfOptimizer:\n",
        "    def __init__(self, obj_function, lower_bound, upper_bound, population_size=5, iterations=20):\n",
        "        self.obj_function = obj_function\n",
        "        self.lower_bound = np.array(lower_bound)\n",
        "        self.upper_bound = np.array(upper_bound)\n",
        "        self.population_size = population_size\n",
        "        self.iterations = iterations\n",
        "        self.dimension = len(lower_bound)\n",
        "\n",
        "    def optimize(self):\n",
        "        wolves = np.random.uniform(self.lower_bound, self.upper_bound, (self.population_size, self.dimension))\n",
        "        alpha, beta, delta = np.zeros(self.dimension), np.zeros(self.dimension), np.zeros(self.dimension)\n",
        "        alpha_score, beta_score, delta_score = float(\"inf\"), float(\"inf\"), float(\"inf\")\n",
        "\n",
        "        for iteration in range(self.iterations):\n",
        "            for i in range(self.population_size):\n",
        "                score = self.obj_function(wolves[i])\n",
        "                if score < alpha_score:\n",
        "                    alpha_score, alpha = score, wolves[i]\n",
        "                elif score < beta_score:\n",
        "                    beta_score, beta = score, wolves[i]\n",
        "                elif score < delta_score:\n",
        "                    delta_score, delta = score, wolves[i]\n",
        "\n",
        "            a = 2 - iteration * (2 / self.iterations)\n",
        "            for i in range(self.population_size):\n",
        "                r1, r2 = np.random.rand(), np.random.rand()\n",
        "                A1, C1 = 2 * a * r1 - a, 2 * r2\n",
        "                D_alpha = abs(C1 * alpha - wolves[i])\n",
        "                X1 = alpha - A1 * D_alpha\n",
        "\n",
        "                r1, r2 = np.random.rand(), np.random.rand()\n",
        "                A2, C2 = 2 * a * r1 - a, 2 * r2\n",
        "                D_beta = abs(C2 * beta - wolves[i])\n",
        "                X2 = beta - A2 * D_beta\n",
        "\n",
        "                r1, r2 = np.random.rand(), np.random.rand()\n",
        "                A3, C3 = 2 * a * r1 - a, 2 * r2\n",
        "                D_delta = abs(C3 * delta - wolves[i])\n",
        "                X3 = delta - A3 * D_delta\n",
        "\n",
        "                wolves[i] = (X1 + X2 + X3) / 3\n",
        "                wolves[i] = np.clip(wolves[i], self.lower_bound, self.upper_bound)\n",
        "\n",
        "        return alpha, alpha_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLH8oXCSnNom",
        "outputId": "7040b35c-57d9-4d5c-cf5e-221be9d2d819"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters Found: [0.00225497]\n",
            "Best Validation Loss Achieved: 0.27273881435394287\n"
          ]
        }
      ],
      "source": [
        "# 5. Define Objective Function for Optimization\n",
        "def objective_function(params):\n",
        "    learning_rate = params[0]\n",
        "    model = segnet()\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=1, batch_size=16, verbose=0)\n",
        "    return history.history['val_loss'][-1]\n",
        "\n",
        "# 6. Run Grey Wolf Optimizer to Find Best Learning Rate\n",
        "gwo = GreyWolfOptimizer(objective_function, lower_bound=[1e-5], upper_bound=[1e-2], population_size=5, iterations=4)\n",
        "best_params, best_score = gwo.optimize()\n",
        "print(\"Best Parameters Found:\", best_params)\n",
        "print(\"Best Validation Loss Achieved:\", best_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncm-_2ypnNpc",
        "outputId": "d8250890-3a9b-496f-cef3-ab3b9adc4a51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 201ms/step - accuracy: 0.6935 - loss: 0.7737 - val_accuracy: 0.7589 - val_loss: 0.3948\n",
            "Epoch 2/50\n",
            "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 183ms/step - accuracy: 0.7301 - loss: 0.5261 - val_accuracy: 0.8354 - val_loss: 0.2773\n",
            "Epoch 3/50\n",
            "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 178ms/step - accuracy: 0.8207 - loss: 0.3058 - val_accuracy: 0.8372 - val_loss: 0.2675\n",
            "Epoch 4/50\n",
            "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 183ms/step - accuracy: 0.8176 - loss: 0.3065 - val_accuracy: 0.8351 - val_loss: 0.2654\n",
            "Epoch 5/50\n",
            "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 181ms/step - accuracy: 0.8259 - loss: 0.2852 - val_accuracy: 0.8427 - val_loss: 0.2453\n",
            "Epoch 6/50\n",
            "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 178ms/step - accuracy: 0.8377 - loss: 0.2585 - val_accuracy: 0.8403 - val_loss: 0.2501\n",
            "Epoch 7/50\n",
            "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.8401 - loss: 0.2571 - val_accuracy: 0.8357 - val_loss: 0.2538\n",
            "Epoch 8/50\n",
            "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 177ms/step - accuracy: 0.8142 - loss: 0.3116 - val_accuracy: 0.8420 - val_loss: 0.2437\n",
            "Epoch 9/50\n",
            "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 181ms/step - accuracy: 0.8418 - loss: 0.2478 - val_accuracy: 0.8492 - val_loss: 0.2294\n",
            "Epoch 10/50\n",
            "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.7768 - loss: 0.6371 - val_accuracy: 0.7847 - val_loss: 0.3687\n",
            "Epoch 11/50\n",
            "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 175ms/step - accuracy: 0.8138 - loss: 0.3030 - val_accuracy: 0.8254 - val_loss: 0.2860\n",
            "Epoch 12/50\n",
            "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 176ms/step - accuracy: 0.8206 - loss: 0.2940 - val_accuracy: 0.8271 - val_loss: 0.2807\n",
            "Epoch 13/50\n",
            "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 175ms/step - accuracy: 0.8277 - loss: 0.2802 - val_accuracy: 0.8280 - val_loss: 0.2725\n",
            "Epoch 14/50\n",
            "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 179ms/step - accuracy: 0.8297 - loss: 0.2761 - val_accuracy: 0.8357 - val_loss: 0.2537\n"
          ]
        }
      ],
      "source": [
        "segnet_model = segnet()\n",
        "segnet_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params[0]),\n",
        "                     loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "history = segnet_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=16,\n",
        "                           callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BZGXbo_Hr63s"
      },
      "outputs": [],
      "source": [
        "best_model=segnet_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nttO0xkMpCtO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import adjusted_rand_score, cohen_kappa_score\n",
        "\n",
        "def jaccard_coef(y_true, y_pred, smooth=1e-10, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Calculate the Jaccard coefficient (Intersection over Union).\n",
        "\n",
        "    Parameters:\n",
        "    - y_true: Ground truth binary mask.\n",
        "    - y_pred: Predicted binary mask.\n",
        "    - smooth: Smoothing factor to avoid division by zero.\n",
        "    - threshold: Threshold to convert predictions to binary.\n",
        "\n",
        "    Returns:\n",
        "    - Jaccard coefficient value.\n",
        "    \"\"\"\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    # Apply threshold to convert predictions to binary\n",
        "    y_pred = (y_pred > threshold).astype(int)\n",
        "\n",
        "    # Flatten the arrays\n",
        "    y_true_f = y_true.flatten()\n",
        "    y_pred_f = y_pred.flatten()\n",
        "\n",
        "    # Calculate intersection and union\n",
        "    intersection = np.sum(y_true_f * y_pred_f)\n",
        "    union = np.sum(y_true_f) + np.sum(y_pred_f) - intersection\n",
        "\n",
        "    # Calculate the Jaccard coefficient (IoU)\n",
        "    jac = (intersection + smooth) / (union + smooth)\n",
        "    return jac\n",
        "\n",
        "\n",
        "def dice_coef(y_true, y_pred, smooth=1e-10, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Calculate the Dice coefficient.\n",
        "\n",
        "    Parameters:\n",
        "    - y_true: Ground truth binary mask.\n",
        "    - y_pred: Predicted binary mask.\n",
        "    - smooth: Smoothing factor to avoid division by zero.\n",
        "    - threshold: Threshold to convert predictions to binary.\n",
        "\n",
        "    Returns:\n",
        "    - Dice coefficient value.\n",
        "    \"\"\"\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    # Apply threshold to convert predictions to binary\n",
        "    y_pred = (y_pred > threshold).astype(int)\n",
        "\n",
        "    # Flatten the arrays\n",
        "    y_true_f = y_true.flatten()\n",
        "    y_pred_f = y_pred.flatten()\n",
        "\n",
        "    # Calculate intersection and sums\n",
        "    intersection = np.sum(y_true_f * y_pred_f)\n",
        "    sum_y_true = np.sum(y_true_f)\n",
        "    sum_y_pred = np.sum(y_pred_f)\n",
        "\n",
        "    # Calculate the Dice coefficient\n",
        "    dice = (2. * intersection + smooth) / (sum_y_true + sum_y_pred + smooth)\n",
        "    return dice\n",
        "\n",
        "\n",
        "def rand_index(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate the Adjusted Rand Index.\n",
        "\n",
        "    Parameters:\n",
        "    - y_true: Ground truth binary mask.\n",
        "    - y_pred: Predicted binary mask.\n",
        "\n",
        "    Returns:\n",
        "    - Adjusted Rand Index value.\n",
        "    \"\"\"\n",
        "    y_true = np.array(y_true).flatten()\n",
        "    y_pred = np.array(y_pred).flatten()\n",
        "    y_pred = (y_pred > 0.5).astype(int)\n",
        "\n",
        "    # Calculate Adjusted Rand Index using scikit-learn\n",
        "    ari = adjusted_rand_score(y_true, y_pred)\n",
        "    return ari\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "def cohen_kappa(y_true, y_pred, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Calculate Cohen's Kappa score.\n",
        "\n",
        "    Parameters:\n",
        "    - y_true: Ground truth binary mask.\n",
        "    - y_pred: Predicted binary mask.\n",
        "    - threshold: Threshold to convert predictions to binary.\n",
        "\n",
        "    Returns:\n",
        "    - Cohen's Kappa score value.\n",
        "    \"\"\"\n",
        "    # Convert both y_true and y_pred to binary\n",
        "    y_true = (np.array(y_true).flatten() > threshold).astype(int)\n",
        "    y_pred = (np.array(y_pred).flatten() > threshold).astype(int)\n",
        "\n",
        "    # Calculate Cohen's Kappa using scikit-learn\n",
        "    kappa = cohen_kappa_score(y_true, y_pred)\n",
        "    return kappa\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ibBFpEyrxaN",
        "outputId": "d37287f1-2100-4091-ea0a-db69d794d404"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 614ms/step\n",
            "Jaccard Coefficient (IoU): 0.8722704127426617\n",
            "Dice Coefficient : 0.9317782375943072\n",
            "Cohen's Kappa  : 0.7544245158443668\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/cluster/_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and binary values for target\n",
            "  warnings.warn(msg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rand index  : 0.6085396321788812\n"
          ]
        }
      ],
      "source": [
        "# Predict on validation set\n",
        "y_pred_val = best_model.predict(X_val)\n",
        "# Calculate Jaccard coefficient (IoU)\n",
        "jaccard_score = jaccard_coef(y_val, y_pred_val)\n",
        "print(f\"Jaccard Coefficient (IoU): {jaccard_score}\")\n",
        "dice_Score=dice_coef(y_val,y_pred_val)\n",
        "print(f\"Dice Coefficient : {dice_Score}\")\n",
        "cohens=cohen_kappa(y_val,y_pred_val)\n",
        "print(f\"Cohen's Kappa  : {cohens}\")\n",
        "rand=rand_index(y_val,y_pred_val)\n",
        "print(f\"Rand index  : {rand}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JcQxm2Lrz8c",
        "outputId": "36fe78e3-ac4a-432c-a87a-14c269993d6e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved successfully!\n"
          ]
        }
      ],
      "source": [
        "best_model.save('/content/drive/MyDrive/best_model_segnet_Antcolony1_maskupdateGreyWolf.h5')  # Save the model to Google Drive or any desired directory\n",
        "\n",
        "print(\"Model saved successfully!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
